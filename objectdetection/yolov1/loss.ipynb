{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2936961874.py, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[70], line 85\u001b[1;36m\u001b[0m\n\u001b[1;33m    torch.flatten(pred_noobject , start_dim=1 ) , torch.flatten((target_noobject , start_dim=1))\u001b[0m\n\u001b[1;37m                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import intersection_over_union \n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"Calculate loss of yolov1\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self , S=7 , B=2 , C=20 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S (int, optional): Grid Size. Defaults to 7.\n",
    "            B (int, optional): number of bounding boxes. Defaults to 2.\n",
    "            C (int, optional): number of classes. Defaults to 20.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        \n",
    "        self.mse = torch.nn.MSELoss(reduction =\"sum\")\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        \n",
    "    def forward(self , predictions , target):\n",
    "        # predictions의 shape = > (BATCH_SIZE , S*S*(C+B*5)) 이므로\n",
    "        # (BATCH_SIZE , S, S, (C+B*5))로 shape 변경\n",
    "        predictions = predictions.reshape(-1 , self.S , self.S , self.C+self.B*5)\n",
    "        # predictions[0:20] => 20개 class\n",
    "        # predictions[20:21] => 첫번째 bouning box의 confidence score\n",
    "        # predictions[21:25] => 첫번째 bouning box 좌표 (x , y , w, h)\n",
    "        # predictions[25:26] => 두번째 bouning box의 confidence score\n",
    "        # predictions[26:30] => 두번째 bouning box 좌표 ( x,y,w,h)\n",
    "        \n",
    "        # B=2인 경우 각 셀 마다 2개의 bounding box가 존재하므로 predictions과 target간의 iou를 계산\n",
    "        iou_b1 = intersection_over_union(predictions[...,21:25], target[...,21:25]) #(N , S ,S , 1)\n",
    "        iou_b2 = intersection_over_union(predictions[...,26:30], target[...,26:30]) #(N , S ,S , 1)\n",
    "        ious = torch.cat([iou_b1.unsquueze(0) , iou_b2.unsquueze(0)], dim=0) #(2,N , S ,S , 1)\n",
    "        \n",
    "        # b1과 b2 중 IOU중 큰 값을 가져오고 , target값의 객체 존재 여부(confidence score 가져옴)\n",
    "        _ , bestbox = torch.max(ious , dim=0)  \n",
    "        # bestbox = > (N , S ,S , 1))\n",
    "        exists_box = target[...:20:21] # (N,S,S,1)\n",
    "        \n",
    "        #======================#\n",
    "        #  FOR BOX COORDIATES  #\n",
    "        #======================#\n",
    "        box_predictions = exists_box*(   # => (N,S,S,1)\n",
    "            # 첫번째 bounding box가 best인 경우\n",
    "            bestbox*predictions[...,26:30] # => (N,S,S,4)\n",
    "            # 두번째 bounding box가 best인 경우\n",
    "            + (1-bestbox)*predictions[...,21:25] # => (N,S,S,4)\n",
    "        )\n",
    "        box_target = exists_box*target[...,21:25] # => (N,S,S,4)       \n",
    "        \n",
    "        # weight , height는 sqrt 취해주기\n",
    "        # 이때 sqrt 안에 음수가 되는 것을 방지하고자 절대값 취하고, 만약에 0이되면 미분시 무한대 발산 방지, \n",
    "        # 또한 abs를 취하면 항상 양수여서 기울기의 방향 정보가 사라지므로 기존의 부호를 정보를 살려주어야 학습진행(torch.sign)\n",
    "        box_predictions[...,2:4] = torch.sign(box_predictions[2:4])*torch.sqrt(torch.abs(box_predictions[2:4]) + 1e-6)\n",
    "        box_target[...,2:4] = torch.sqrt(box_target[...,2:4])\n",
    "        \n",
    "        # coorediate loss 구하기 (N,S,S,4) => (N*S*S,4) 로 변경 후 prediction과 target간의 mse 계산\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions , end_dim=-2) , torch.flatten(box_target , end_dim = -2)\n",
    "        )\n",
    "        \n",
    "        #=====================#\n",
    "        #   FOR OBJECT LOSS   #\n",
    "        #=====================#\n",
    "        # confidenc socre\n",
    "        # (N,S,S,1) => (N*S*S,1)\n",
    "        pred_confidence = exists_box*(bestbox*(predictions[...,25:26]) + (1-bestbox)*(predictions[...,20:21]))\n",
    "        target_confidence = exists_box*target[...,20:21]\n",
    "        \n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(pred_confidence , end_dim=-2) , torch.flatten(target_confidence, end_dim=-2)\n",
    "        )\n",
    "        #========================#\n",
    "        #   FOR NO OBJECT LOSS   #\n",
    "        #========================#\n",
    "        # (N , S , S , 1) =>(N , S*S*1)\n",
    "        pred_noobject = (1-exists_box)*(predictions[...,25:26]) + (1-exists_box)*(predictions[...,20:21])\n",
    "        target_noobject = (1-exists_box)*target[...,20:21] + (1-exists_box)*target[...,20:21] \n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten(pred_noobject , start_dim=1 ) , torch.flatten(target_noobject , start_dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #====================#\n",
    "        #   FOR CLASS LOSS   #\n",
    "        #====================#\n",
    "        \n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box*predictions[...,:20] , end_dim=-2),\n",
    "            torch.flatten(exists_box*target[...,:20] , end_dim=-2)\n",
    "        )\n",
    "        \n",
    "        loss = (\n",
    "            self.lambda_coord*box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_noobj*no_object_loss\n",
    "            + class_loss\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting loss.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loss.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import intersection_over_union \n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"Calculate loss of yolov1\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self , S=7 , B=2 , C=20 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S (int, optional): Grid Size. Defaults to 7.\n",
    "            B (int, optional): number of bounding boxes. Defaults to 2.\n",
    "            C (int, optional): number of classes. Defaults to 20.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        \n",
    "        self.mse = torch.nn.MSELoss(reduction =\"sum\")\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        \n",
    "    def forward(self , predictions , target):\n",
    "        # predictions의 shape = > (BATCH_SIZE , S*S*(C+B*5)) 이므로\n",
    "        # (BATCH_SIZE , S, S, (C+B*5))로 shape 변경\n",
    "        predictions = predictions.reshape(-1 , self.S , self.S , self.C+self.B*5)\n",
    "        # predictions[0:20] => 20개 class\n",
    "        # predictions[20:21] => 첫번째 bouning box의 confidence score\n",
    "        # predictions[21:25] => 첫번째 bouning box 좌표 (x , y , w, h)\n",
    "        # predictions[25:26] => 두번째 bouning box의 confidence score\n",
    "        # predictions[26:30] => 두번째 bouning box 좌표 ( x,y,w,h)\n",
    "        \n",
    "        # B=2인 경우 각 셀 마다 2개의 bounding box가 존재하므로 predictions과 target간의 iou를 계산\n",
    "        iou_b1 = intersection_over_union(predictions[...,21:25], target[...,21:25]) #(N , S ,S , 1)\n",
    "        iou_b2 = intersection_over_union(predictions[...,26:30], target[...,26:30]) #(N , S ,S , 1)\n",
    "        ious = torch.cat([iou_b1.unsquueze(0) , iou_b2.unsquueze(0)], dim=0) #(2,N , S ,S , 1)\n",
    "        \n",
    "        # b1과 b2 중 IOU중 큰 값을 가져오고 , target값의 객체 존재 여부(confidence score 가져옴)\n",
    "        _ , bestbox = torch.max(ious , dim=0)  \n",
    "        # bestbox = > (N , S ,S , 1))\n",
    "        exists_box = target[...:20:21] # (N,S,S,1)\n",
    "        \n",
    "        #======================#\n",
    "        #  FOR BOX COORDIATES  #\n",
    "        #======================#\n",
    "        box_predictions = exists_box*(   # => (N,S,S,1)\n",
    "            # 첫번째 bounding box가 best인 경우\n",
    "            bestbox*predictions[...,26:30] # => (N,S,S,4)\n",
    "            # 두번째 bounding box가 best인 경우\n",
    "            + (1-bestbox)*predictions[...,21:25] # => (N,S,S,4)\n",
    "        )\n",
    "        box_target = exists_box*target[...,21:25] # => (N,S,S,4)       \n",
    "        \n",
    "        # weight , height는 sqrt 취해주기\n",
    "        # 이때 sqrt 안에 음수가 되는 것을 방지하고자 절대값 취하고, 만약에 0이되면 미분시 무한대 발산 방지, \n",
    "        # 또한 abs를 취하면 항상 양수여서 기울기의 방향 정보가 사라지므로 기존의 부호를 정보를 살려주어야 학습진행(torch.sign)\n",
    "        box_predictions[...,2:4] = torch.sign(box_predictions[2:4])*torch.sqrt(torch.abs(box_predictions[2:4]) + 1e-6)\n",
    "        box_target[...,2:4] = torch.sqrt(box_target[...,2:4])\n",
    "        \n",
    "        # coorediate loss 구하기 (N,S,S,4) => (N*S*S,4) 로 변경 후 prediction과 target간의 mse 계산\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions , end_dim=-2) , torch.flatten(box_target , end_dim = -2)\n",
    "        )\n",
    "        \n",
    "        #=====================#\n",
    "        #   FOR OBJECT LOSS   #\n",
    "        #=====================#\n",
    "        # confidenc socre\n",
    "        # (N,S,S,1) => (N*S*S,1)\n",
    "        pred_confidence = exists_box*(bestbox*(predictions[...,25:26]) + (1-bestbox)*(predictions[...,20:21]))\n",
    "        target_confidence = exists_box*target[...,20:21]\n",
    "        \n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(pred_confidence , end_dim=-2) , torch.flatten(target_confidence, end_dim=-2)\n",
    "        )\n",
    "        #========================#\n",
    "        #   FOR NO OBJECT LOSS   #\n",
    "        #========================#\n",
    "        # (N , S , S , 1) =>(N , S*S*1)\n",
    "        pred_noobject = (1-exists_box)*(predictions[...,25:26]) + (1-exists_box)*(predictions[...,20:21])\n",
    "        target_noobject = (1-exists_box)*target[...,20:21] + (1-exists_box)*target[...,20:21] \n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten(pred_noobject , start_dim=1 ) , torch.flatten(target_noobject , start_dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #====================#\n",
    "        #   FOR CLASS LOSS   #\n",
    "        #====================#\n",
    "        \n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box*predictions[...,:20] , end_dim=-2),\n",
    "            torch.flatten(exists_box*target[...,:20] , end_dim=-2)\n",
    "        )\n",
    "        \n",
    "        loss = (\n",
    "            self.lambda_coord*box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_noobj*no_object_loss\n",
    "            + class_loss\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import torch\n",
    "def intersection_over_union(boxes_preds , boxes_labels , box_format = \"midpoint\"):\n",
    "    \"\"\"Calculate IOU(intersection_over_union)\n",
    "\n",
    "    Args:\n",
    "        boxes_preds (tensor): Prediction of Bounding boxes (Batch_size , 4)\n",
    "        boxes_labels (tensor): Correct labels of Bouning boxes(Batch_size , 4)\n",
    "        box_format (str, optional): bouning box format midpoint or corners Defaults to \"midpoint\".\n",
    "        \n",
    "    Returns :\n",
    "        tensor : (... , 1) => 2 입력 Tensor bounding box의 IOU 반환\n",
    "    \"\"\"\n",
    "    if box_format ==\"midpoint\": # [center x , center y , width , height]\n",
    "        # convert midpoint format => corners format [min x , min y , max x , max y ]\n",
    "        box1_x1 = boxes_preds[...,0:1] - boxes_preds[...,2:3]/2\n",
    "        box1_y1 = boxes_preds[...,1:2] - boxes_preds[...,3:4]/2\n",
    "        box1_x2 = boxes_preds[...,0:1] + boxes_preds[...,2:3]/2\n",
    "        box1_y2 = boxes_preds[...,1:2] + boxes_preds[...,3:4]/2\n",
    "        \n",
    "        box2_x1 = boxes_labels[...,0:1] - boxes_labels[...,2:3]/2\n",
    "        box2_y1 = boxes_labels[...,1:2] - boxes_labels[...,3:4]/2\n",
    "        box2_x2 = boxes_labels[...,0:1] + boxes_labels[...,2:3]/2\n",
    "        box2_y2 = boxes_labels[...,1:2] + boxes_labels[...,3:4]/2\n",
    "        \n",
    "    if box_format ==\"corners\": # [min x , min y , max x , max y ]\n",
    "        box1_x1 = boxes_preds[...,0:1]\n",
    "        box1_y1 = boxes_preds[...,1:2]\n",
    "        box1_x2 = boxes_preds[...,2:3]\n",
    "        box1_y2 = boxes_preds[...,3:4]\n",
    "        \n",
    "        box2_x1 = boxes_labels[...,0:1]\n",
    "        box2_y1 = boxes_labels[...,1:2]\n",
    "        box2_x2 = boxes_labels[...,2:3]\n",
    "        box2_y2 = boxes_labels[...,3:4]\n",
    "    \n",
    "    # Calculate IOU\n",
    "    # box1과 box2의 공통된 영역의 x1 y1 x2 y2 계산\n",
    "    x1 = torch.max(box1_x1 , box2_x1) # => (... , 1)\n",
    "    y1 = torch.max(box1_y1 , box2_y1) # => (... , 1)\n",
    "    x2 = torch.min(box1_x2 , box2_x2) # => (... , 1)\n",
    "    y2 = torch.min(box1_y2 , box2_y2) # => (... , 1)\n",
    "    # torch.clamp => 두 bounding box의 교집합이 없는 경우 (x2-x1) < 0 이고, (y2-y1) < 0 가 되는데\n",
    "    # clamp를 사용하여 0보다 작으면 0으로 변경하는 연산\n",
    "    intersection = (x2-x1).clamp(0)*(y2-y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1)*(box1_y2-box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1)*(box2_y2-box2_y1))\n",
    "    \n",
    "    return intersection / (box1_area+box2_area - intersection + 1e-6)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.5132e-01,  4.6952e-01,  1.5760e+00,  1.8567e+00],\n",
       "         [ 5.0284e-01, -1.8092e+00,  5.7576e-01, -1.3015e+00],\n",
       "         [ 1.2145e-01,  4.2196e-02, -9.9300e-02, -1.5776e-02],\n",
       "         [ 2.8582e+00,  1.8284e+00, -1.9579e+00, -2.2795e-01],\n",
       "         [-7.4400e-01,  4.5556e-01,  3.9584e-01, -4.9630e-01],\n",
       "         [-3.1231e-01,  1.0872e+00,  6.6775e-01, -8.0256e-01],\n",
       "         [-8.5357e-01,  3.7244e-01, -4.1710e-02,  8.0324e-01],\n",
       "         [ 1.6985e+00, -3.4034e-01,  1.3576e-02, -1.2326e+00],\n",
       "         [ 1.6847e-01,  6.0962e-02,  6.6231e-02, -3.1844e-01],\n",
       "         [ 4.1069e-02, -6.7639e-02, -2.2936e-02,  5.0985e-02],\n",
       "         [-1.8575e+00, -6.8541e-01,  2.0984e-01,  8.1979e-01],\n",
       "         [ 9.9836e-02,  2.8077e-04,  1.7064e-01, -1.0605e-01],\n",
       "         [ 1.2838e-01, -9.9127e-01,  4.8531e-01, -7.5959e-01],\n",
       "         [-5.8952e-01, -1.5339e-01,  9.9495e-03, -2.4602e-01],\n",
       "         [ 3.6586e-01, -7.6267e-01, -2.2626e-02, -1.8047e+00],\n",
       "         [-5.1042e-01, -1.4695e-01,  2.5315e-01, -2.1208e-01],\n",
       "         [ 5.2512e-02, -3.6604e-01,  2.4055e-03,  1.5897e-02],\n",
       "         [ 9.9304e-01, -8.0220e-01, -3.0492e-01, -2.7098e-01],\n",
       "         [ 5.3437e-01,  3.3508e-01, -1.7734e-01,  5.7814e-02],\n",
       "         [-1.5842e-01, -1.2420e-01,  2.1388e-02, -1.0271e-01]],\n",
       "\n",
       "        [[-3.7539e-01,  1.9281e-01,  1.0991e-01, -6.1139e-02],\n",
       "         [-1.0344e-02,  1.0920e+00,  4.4409e-01,  4.8817e-01],\n",
       "         [ 8.8023e-01,  6.8530e-01,  7.6484e-01,  1.2368e+00],\n",
       "         [ 1.1864e-01,  2.6482e-03, -4.1742e-01,  8.4502e-02],\n",
       "         [ 3.8754e-01,  1.4256e-01, -1.6268e+00, -7.5161e-01],\n",
       "         [-2.6152e-01, -4.3943e-01, -1.1199e+00,  6.3035e-01],\n",
       "         [-3.4909e-01, -1.0763e+00,  3.3759e-01, -7.9296e-01],\n",
       "         [ 1.4672e-01, -1.1808e-01,  1.9400e-01, -7.0550e-02],\n",
       "         [-5.7133e-01,  2.1359e-01,  8.6958e-01,  2.0696e-03],\n",
       "         [ 5.3585e-01, -3.7289e-01, -3.1255e-01,  5.2844e-01],\n",
       "         [-1.3264e-01,  2.1170e-02, -1.9177e-01,  2.2599e-03],\n",
       "         [ 4.6504e-01,  8.0699e-01, -1.6329e+00, -1.6003e+00],\n",
       "         [ 5.4279e-02, -1.6141e+00, -5.7652e-01,  7.4787e-01],\n",
       "         [ 5.3906e-01, -9.7558e-01, -2.9380e-01, -6.6504e-01],\n",
       "         [-3.2504e-02,  6.3864e-02, -3.8364e-02, -4.1066e-02],\n",
       "         [ 7.9923e-01, -5.8146e-01, -1.5824e-01, -3.9242e-01],\n",
       "         [-1.3619e+00, -5.3624e-01, -8.5616e-01,  1.3458e+00],\n",
       "         [-1.0299e-01,  9.5137e-01, -1.0854e-01, -4.0732e-01],\n",
       "         [ 1.8119e-01, -7.5274e-01, -9.1113e-01, -3.5851e-01],\n",
       "         [ 4.0871e-02,  2.6212e-01,  1.5807e-01, -3.3295e-01]],\n",
       "\n",
       "        [[ 5.0763e-01,  5.5215e-01,  2.1658e+00, -8.3385e-01],\n",
       "         [ 1.1638e-01, -7.9204e-02,  5.6570e-02,  9.7342e-02],\n",
       "         [ 2.0058e-01,  9.8902e-01, -7.4103e-01,  1.1444e-02],\n",
       "         [-7.6231e-02, -6.7072e-01, -2.7329e-01,  3.0943e-01],\n",
       "         [-1.2943e-01, -1.4639e+00,  3.0605e+00, -8.4307e-01],\n",
       "         [ 6.7355e-01,  3.1152e-01, -3.8624e-01,  8.8891e-01],\n",
       "         [ 1.2508e-01,  1.8485e-01,  1.2181e-01, -6.6146e-01],\n",
       "         [-3.0595e+00, -1.9082e+00, -3.6854e+00,  1.0880e+00],\n",
       "         [-1.1579e+00,  4.6217e-02, -1.5715e+00, -3.3744e-01],\n",
       "         [ 1.0914e-03, -3.3213e-02, -2.9371e-03,  1.3225e-01],\n",
       "         [ 2.4545e-01, -7.2110e-01,  5.4698e-01, -4.3271e-01],\n",
       "         [-8.7393e-01, -6.4199e-02,  2.4839e-01, -8.9297e-01],\n",
       "         [-1.3737e+00, -2.4998e-01, -3.4036e-01,  1.3320e+00],\n",
       "         [ 1.4997e+00, -8.3060e-01,  2.0326e-01,  2.7736e-03],\n",
       "         [-9.5194e-02,  1.2620e-01,  1.3544e-01, -1.4151e-01],\n",
       "         [-8.8298e-03,  1.8776e-01, -1.7693e-01,  1.3999e-01],\n",
       "         [ 5.4166e-01, -7.5791e-02,  8.5482e-03, -2.4494e+00],\n",
       "         [ 9.8331e-02,  2.3502e-01, -5.9087e-01, -3.0712e-01],\n",
       "         [-3.8830e-01,  1.3489e+00,  1.4166e+00,  3.3559e-01],\n",
       "         [-1.5258e+00,  4.5366e-01, -1.2635e+00,  3.1503e-02]],\n",
       "\n",
       "        [[-1.5830e-01,  2.6223e+00,  3.4585e+00, -2.2511e+00],\n",
       "         [-8.2429e-01,  1.5014e-01, -7.9700e-01, -6.4405e-01],\n",
       "         [-1.8071e-02,  1.2966e+00,  2.2917e-01, -2.8470e-01],\n",
       "         [-3.1761e-01,  6.5469e-01, -2.8889e-01,  9.5306e-02],\n",
       "         [ 1.1709e-01,  1.4447e-01,  2.5335e-01,  8.3252e-03],\n",
       "         [ 1.7799e-02, -8.3524e-02, -1.2628e-02, -6.9688e-02],\n",
       "         [-5.8225e-01,  5.4221e-01,  1.3230e+00, -3.3080e+00],\n",
       "         [-1.7845e+00, -7.3001e-01,  2.7420e-01,  1.4784e+00],\n",
       "         [-2.3636e-01,  1.0495e+00, -1.5191e+00, -1.9115e+00],\n",
       "         [ 1.1008e+00,  1.7422e-01,  3.0090e-01,  4.9462e-01],\n",
       "         [-8.0479e-01, -1.1269e+00, -9.1463e-01, -1.3041e-01],\n",
       "         [ 3.7777e-01, -9.8756e-01, -8.9494e-01, -1.5813e+00],\n",
       "         [ 5.4760e-02, -5.6025e-01, -1.9690e-01,  1.5526e-01],\n",
       "         [-1.1066e-01,  1.8238e+00,  1.1878e+00, -1.6600e-01],\n",
       "         [-6.4009e-02, -7.6212e-02,  2.3414e-01,  5.8736e-02],\n",
       "         [ 1.4878e-01,  5.4322e-02,  2.9403e-01,  1.9646e-01],\n",
       "         [ 1.7319e+00, -9.5122e-01,  2.3189e-01, -5.8085e-01],\n",
       "         [ 2.6230e-02, -2.0854e-01,  2.8547e-01,  6.0173e-01],\n",
       "         [-2.1411e-02, -2.2175e-01,  2.2088e-01, -1.0342e-01],\n",
       "         [-6.0745e-01,  1.9399e+00,  6.5771e-02,  1.2588e+00]],\n",
       "\n",
       "        [[-6.5809e-02, -2.9919e-02, -1.3974e-02,  9.1735e-02],\n",
       "         [-2.5945e+00, -1.2102e+00,  2.8562e+00,  1.3504e+00],\n",
       "         [-3.9766e-01,  6.3858e-01, -9.7517e-01,  1.7943e-01],\n",
       "         [-1.8901e-01, -5.1178e-02,  5.6278e-02,  3.3648e-02],\n",
       "         [-8.2619e-01,  3.8935e-01,  8.0670e-02,  3.8387e-01],\n",
       "         [-1.2709e-01, -3.2558e+00, -4.7169e-01, -1.7831e+00],\n",
       "         [-1.3388e-05, -3.0381e-03, -1.6794e-03, -7.8592e-04],\n",
       "         [-8.0622e-01, -1.7590e-01,  4.3362e-03,  6.7018e-01],\n",
       "         [-4.6187e-01, -2.9826e-01, -1.9656e-01,  3.5956e-01],\n",
       "         [ 8.9146e-01,  2.4247e-01,  9.0484e-01,  1.0880e+00],\n",
       "         [ 7.5021e-02, -1.0477e+00,  1.0620e+00, -8.0775e-01],\n",
       "         [ 1.4833e-01, -6.9815e-01,  8.4298e-02,  7.0239e-01],\n",
       "         [-1.9524e+00, -8.7096e-01,  4.7494e-01, -4.3419e-01],\n",
       "         [ 3.8691e-02,  4.5694e-02, -8.1641e-02,  1.9919e-02],\n",
       "         [ 6.5437e-01, -3.4722e-01,  1.9789e+00,  1.3997e-01],\n",
       "         [ 3.0267e-01, -1.3270e+00, -1.9453e+00, -7.6939e-01],\n",
       "         [ 4.3666e-01,  2.2699e-01, -1.0064e-02, -1.3260e-01],\n",
       "         [-7.2509e-01, -1.8591e-01,  7.2087e-02, -1.0931e-01],\n",
       "         [-1.2247e-01,  9.4615e-01,  1.2034e+00,  7.7071e-01],\n",
       "         [-5.3709e-01, -1.2426e-01,  3.7948e-02, -2.9988e-01]],\n",
       "\n",
       "        [[ 1.4398e+00, -9.2019e-01, -6.1346e-01,  1.2716e+00],\n",
       "         [ 8.8793e-01, -8.3046e-01,  1.5045e+00,  8.5948e-01],\n",
       "         [-1.5421e+00, -1.1425e+00, -1.2964e+00, -6.3923e-02],\n",
       "         [ 5.5284e-01, -1.3657e-01,  2.4920e-01, -8.8585e-03],\n",
       "         [-1.6215e+00, -1.5452e+00, -1.1375e+00, -2.8769e+00],\n",
       "         [-1.2700e+00, -5.9049e-02,  5.9492e-03,  1.0717e+00],\n",
       "         [-3.6457e-01,  6.3247e-01,  1.1042e-01, -5.5160e-01],\n",
       "         [-1.7390e+00, -2.9288e+00,  3.1408e+00, -4.3391e+00],\n",
       "         [-8.3817e-01, -2.9994e+00, -1.0411e+00,  3.8092e-01],\n",
       "         [-8.8128e-01,  2.0920e-01,  8.5184e-01,  1.2290e+00],\n",
       "         [-8.4612e-03,  1.9266e-02,  5.3603e-03,  6.5487e-04],\n",
       "         [-5.2068e-01, -3.2529e-01, -4.6179e-01,  2.1357e-01],\n",
       "         [ 6.5239e-01, -6.2112e-01, -9.1542e-01, -1.3401e+00],\n",
       "         [-3.3567e-02, -1.1015e-01, -6.2107e-02,  5.1069e-03],\n",
       "         [ 5.7889e-02,  1.6125e-01, -3.4786e-02, -1.4587e-01],\n",
       "         [-6.5682e-02, -9.4768e-01,  1.6012e-01, -2.4745e-01],\n",
       "         [ 2.9840e+00, -4.0471e-01, -1.7819e+00, -6.9843e-01],\n",
       "         [-6.4442e-02,  2.3253e+00,  4.4632e-01, -1.0866e+00],\n",
       "         [-5.8597e-02,  6.2647e-01,  1.6417e-01, -2.9264e-01],\n",
       "         [ 4.3598e-01, -6.4131e-01, -4.4229e-01, -1.6223e-01]],\n",
       "\n",
       "        [[-1.8578e-01,  2.7658e-01, -2.2324e-01, -6.8556e-02],\n",
       "         [-3.1454e-01, -2.5389e+00,  6.7524e-01, -7.7278e-01],\n",
       "         [ 5.4792e-01,  2.0883e-01,  2.7698e+00, -2.0509e-01],\n",
       "         [-8.8197e-03,  1.5275e-01, -6.6951e-01,  9.8248e-01],\n",
       "         [-5.0388e-02, -1.3729e-01,  4.1115e-01, -1.6008e-01],\n",
       "         [ 6.3226e-01, -5.7640e-01, -3.6651e-01, -1.0193e+00],\n",
       "         [-4.6182e-01,  2.4422e-01,  3.3463e-01,  3.8196e-01],\n",
       "         [ 6.6035e-01,  5.4184e-01, -9.6292e-01, -1.9764e+00],\n",
       "         [ 3.3343e-02,  2.6186e-01, -7.1782e-03, -4.0310e-02],\n",
       "         [-3.8443e-01,  4.3795e-01, -5.5197e-01,  1.4234e+00],\n",
       "         [-9.9751e-01, -9.6965e-01,  2.6280e+00, -1.1981e+00],\n",
       "         [ 1.3295e-01,  3.8075e-01, -1.6388e-01, -5.5286e-02],\n",
       "         [ 2.4367e-01,  5.2883e-01, -4.0200e-01,  2.2547e-01],\n",
       "         [-2.2654e-01,  1.5866e-01,  2.1009e-01, -3.9853e-01],\n",
       "         [-2.5105e-01,  9.6251e-01,  1.1256e+00,  1.7840e+00],\n",
       "         [ 3.0318e-01, -1.3405e+00, -8.1795e-01, -9.8410e-01],\n",
       "         [ 1.4016e+00,  7.4799e-01, -1.4068e+00,  3.5024e-01],\n",
       "         [ 4.6876e-01, -2.1280e-01, -7.6715e-01, -5.3867e-01],\n",
       "         [ 1.2511e+00,  2.0551e+00,  2.0986e+00,  2.2356e-01],\n",
       "         [ 1.2859e-02,  1.3495e-01, -4.6706e-02, -7.0506e-02]],\n",
       "\n",
       "        [[-1.2089e-01,  1.2147e-01,  4.6319e-01,  1.7025e-01],\n",
       "         [-6.1211e-01,  4.8580e-01, -4.7700e-01,  8.1639e-01],\n",
       "         [ 1.0415e+00,  5.7744e-01,  7.3320e-01, -1.8983e+00],\n",
       "         [-5.9957e-01,  6.6579e-03,  1.3951e-01,  1.7279e-01],\n",
       "         [ 1.8362e-03, -1.9777e-02,  2.2777e-02,  3.3380e-03],\n",
       "         [ 1.1874e-01, -1.7273e-01, -1.5067e-01, -2.2070e-01],\n",
       "         [-1.1777e+00, -3.9882e-01, -7.0850e-01,  1.4391e+00],\n",
       "         [-4.4717e-01,  7.0921e-01,  1.7678e-01,  3.2988e-01],\n",
       "         [-2.3603e-01, -1.0397e+00, -1.1747e+00,  4.0448e-01],\n",
       "         [ 8.9789e-02, -1.5944e+00,  2.5324e+00,  5.9407e-01],\n",
       "         [-4.8826e-01,  8.7333e-01, -2.1082e-01,  1.1138e+00],\n",
       "         [ 7.5542e-05, -5.4864e-03, -2.4896e-02,  4.0819e-02],\n",
       "         [-2.2305e-01,  1.2668e+00,  1.2139e-01,  8.6890e-01],\n",
       "         [ 1.3744e+00, -1.8535e+00, -2.7952e-01, -1.4051e+00],\n",
       "         [ 1.3051e+00,  2.2556e-01,  2.4649e+00, -3.9574e-01],\n",
       "         [-1.1508e-01,  8.0449e-01,  2.2880e-01, -8.0606e-01],\n",
       "         [-4.9471e-02, -8.1894e-01,  3.7021e-01,  7.5776e-01],\n",
       "         [-1.5462e+00, -3.9558e-01, -1.5125e+00,  1.8109e-01],\n",
       "         [-2.7775e-01,  1.8318e-01, -5.1242e-01, -2.0986e-02],\n",
       "         [-6.3214e-01,  6.5196e-01, -3.9405e-01,  9.7067e-01]],\n",
       "\n",
       "        [[ 1.1629e+00, -7.1659e-01,  1.2028e+00,  4.5915e-01],\n",
       "         [ 1.4309e+00,  5.9196e-01,  1.0210e+00, -8.0163e-01],\n",
       "         [-4.0658e-01,  2.4432e-01,  9.3526e-02,  2.3162e-02],\n",
       "         [-3.2374e-02, -3.0739e-02,  4.5126e-02, -3.9056e-02],\n",
       "         [-2.8497e-01,  9.6367e-01, -1.4686e-01, -5.0972e-02],\n",
       "         [ 6.0685e-01, -2.7176e-01, -5.2319e-01,  8.6243e-01],\n",
       "         [ 8.6367e-01,  1.9734e+00,  2.1159e+00, -4.8054e-02],\n",
       "         [-2.5293e-01,  3.8293e-02,  2.5386e-01,  1.9066e-01],\n",
       "         [ 4.2530e-03, -3.0525e-03, -2.8101e-02, -2.7322e-02],\n",
       "         [-6.6069e-03, -2.0492e-02, -1.3351e-02,  1.2501e-02],\n",
       "         [ 3.2127e-01,  3.6086e-01,  3.2622e-01, -2.2566e-01],\n",
       "         [ 6.4614e-02, -2.0635e+00,  3.4344e+00, -2.1872e+00],\n",
       "         [ 2.4568e-01, -2.3691e-01,  3.3137e-01, -5.9552e-01],\n",
       "         [-8.8565e-01, -9.5223e-01, -6.3022e-01, -2.5409e-01],\n",
       "         [ 2.5519e-01,  2.6292e+00,  1.9463e+00, -8.7001e-01],\n",
       "         [ 4.4050e-01,  1.7603e+00,  6.3441e-02, -3.2951e+00],\n",
       "         [ 6.2154e-01, -3.0926e+00, -2.0281e+00, -2.9124e-01],\n",
       "         [ 3.5494e-01,  5.1606e-01, -9.0125e-01, -1.5028e+00],\n",
       "         [ 8.0446e-02,  2.8039e-02,  2.9610e-03, -2.8809e-02],\n",
       "         [ 1.4574e+00, -2.0201e-01,  8.9264e-01, -1.9827e-01]],\n",
       "\n",
       "        [[-3.9044e-01, -3.8886e-02,  5.2442e-02, -2.1795e-01],\n",
       "         [-1.6012e+00,  7.9421e-01, -7.8407e-01,  4.7322e-02],\n",
       "         [ 1.2987e+00, -1.0347e+00, -4.5182e-01, -8.0171e-01],\n",
       "         [ 2.8116e-01, -9.5874e-02, -5.9058e-01, -1.3844e-01],\n",
       "         [-2.4837e-02, -3.4113e-01, -4.5449e-01,  5.2411e-01],\n",
       "         [ 2.6266e-01,  1.5638e+00,  1.4222e+00,  1.3010e+00],\n",
       "         [ 1.9383e-01,  1.3634e+00,  8.3191e-01, -1.6701e-01],\n",
       "         [-7.9896e-02,  1.0376e+00, -1.0020e+00, -6.1385e-01],\n",
       "         [ 7.3422e-01,  1.1618e+00, -1.4790e+00, -4.1153e-01],\n",
       "         [ 5.3978e-01,  4.6067e-03, -1.3497e-01, -6.5349e-01],\n",
       "         [-2.0914e-01,  1.2898e+00, -3.8672e-01, -4.7490e-01],\n",
       "         [ 9.2312e-01, -5.8404e-01, -3.5784e-01,  8.6029e-01],\n",
       "         [-8.1622e-01,  5.8647e-01, -1.0497e+00,  1.2309e+00],\n",
       "         [ 4.1173e-01, -1.5585e+00,  2.8987e-01, -1.1632e-01],\n",
       "         [ 4.5174e-01, -2.0703e-01,  4.6071e-01, -2.3320e-01],\n",
       "         [ 2.5665e-01, -1.7927e+00,  2.6668e+00,  2.0131e+00],\n",
       "         [-5.1049e-01, -2.6658e+00, -1.4783e+00, -4.1809e-01],\n",
       "         [-6.8141e-02, -3.8510e-01, -5.4727e-01,  1.5064e-01],\n",
       "         [-6.4118e-02,  2.8095e-02,  4.4692e-02, -8.3237e-02],\n",
       "         [ 1.3062e+00, -2.7699e+00,  2.9135e-01,  2.3141e-01]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10,20,1) *torch.randn(10,20,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "import torch\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self , csv_file , S=7, B=2, C=20 , transforms=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.transforms = transforms    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, x):\n",
    "        image_path = self.df.iloc[x,0]\n",
    "        label_path = self.df.iloc[x,1]\n",
    "        boxes = []\n",
    "        with open(label_path , \"r\") as f:\n",
    "            for object in f.readlines():\n",
    "                class_label , x, y , w, h = [int(i) if float(i)==int(float(i)) else float(i)  for i in object.strip().split(\" \")]\n",
    "                boxes.append((class_label, x , y , w, h))\n",
    "        img = Image.open(image_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img , boxes = self.transforms(img , boxes)\n",
    "        label_matrix = torch.zeros(self.S , self.S , self.C+self.B*5)\n",
    "        # 앞에서 구한 x,y,w,h가 셀을 기준으로 어디에 포함되는 구하고, 셀을 기준으로 x,y,w,h 계산\n",
    "        for box in boxes:\n",
    "            class_label , x , y , w , h = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "            # i,j => SxS에서의 객체 중심점이 위치한 행, 열 정보\n",
    "            i,j = int(self.S*y) , int(self.S*x)\n",
    "            # 셀 기준 중심점 위치\n",
    "            x_cell , y_cell = self.S*x - j , self.S*y - i\n",
    "            # 셀 기준 bounding box width , hegith\n",
    "            width_cell , height_cell = self.S*w , self.S*h\n",
    "            # label_matrix = [7,7,30] => 0~19는 class확률 , 20 : confidence ,21~24 bounding box, 25 : confidence , 26~29 : boundingbox \n",
    "            if label_matrix[i,j,20] == 0:\n",
    "                label_matrix[i,j,20]=1\n",
    "                box_coordinates = torch.tensor([x_cell , y_cell , width_cell , height_cell])\n",
    "                label_matrix[i,j,21:25] = box_coordinates\n",
    "                label_matrix[i,j,class_label] = 1            \n",
    "        return img , label_matrix\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5011"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import VOCDataset\n",
    "train_dataset = VOCDataset(\"./data/train.csv\")\n",
    "test_dataset = VOCDataset(\"./data/test.csv\")\n",
    "img , label = train_dataset[0]\n",
    "img2 , label2 = test_dataset[0]\n",
    "\n",
    "len(train_dataset) + len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m label_matrix\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m boxes:\n\u001b[1;32m---> 15\u001b[0m     class_label , x , y , w, h \u001b[38;5;241m=\u001b[39m \u001b[43mbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/test.csv\" )\n",
    "txt = df.iloc[1,1]\n",
    "img = df.iloc[1,0]\n",
    "boxes = []\n",
    "with open(txt , \"r\") as f:\n",
    "    for label in f.readlines():\n",
    "        class_label , x, y, w,h = [ int(i) if float(i) == int(float(i)) else float(i) for i in label.strip().split(\" \") ]\n",
    "        boxes.append((class_label , x , y , w, h))\n",
    "img = Image.open(img)\n",
    "boxes = torch.tensor(boxes)\n",
    "\n",
    "label_matrix = torch.zeros(7,7,30)\n",
    "label_matrix.shape\n",
    "for box in boxes:\n",
    "    class_label , x , y , w, h = box.tolist()\n",
    "    class_label = int(class_label)\n",
    "    # 중심점이 7x7에서 어느 행,열에 속하는지(i,j)\n",
    "    i, j = int(7*y) , int(7*x)\n",
    "    # cell 기준으로 하여 중심점 위치 \n",
    "    x_cell , y_cell = 7*x - j  , 7*y - i\n",
    "    # 7x7 기준 이미지 width ,height \n",
    "    width_cell , height_cell = w*7 , h*7\n",
    "    # label_matrix = [7,7,30] => 0~19는 class확률 , 20 : confidence ,21~24 bounding box, 25 : confidence , 26~29 : boundingbox \n",
    "    if label_matrix[i,j,20] == 0:\n",
    "        label_matrix[i,j,20] = 1\n",
    "        box_coordinates = torch.tensor([x_cell , y_cell , width_cell , height_cell])\n",
    "        label_matrix[i,j,21:25] = box_coordinates\n",
    "        label_matrix[i,j,25] = 1\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i if i > 5 else 0 for i in range(10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(3) == int(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NMS 계산\n",
    "import torch\n",
    "from collections import Counter\n",
    "from utils import intersection_over_union\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes:list,\n",
    "    true_boxes,\n",
    "    iou_threshold=0.5,\n",
    "    box_format=\"corners\",\n",
    "    num_classes = 20\n",
    "):\n",
    "    # pred_boxes = [[train_idx , class_pred , prob_score , x1,y1,x2,y2] , [] ,...]\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6 \n",
    "    \n",
    "    # 클래스 갯수 만큼 AP 계산\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        # 현재 클래스 번호로 예측한 것과 정답을 가져옴\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(detection)\n",
    "\n",
    "        # 각 이미지 번호에 대하여 c클래스인 object가 몇개 있는지 확인\n",
    "        # img0 has 3 c class\n",
    "        # img1 has 5 c class\n",
    "        # {0 :3 , 1:5}\n",
    "        amount_bboxes = Counter(gt[0] for gt in ground_truths)\n",
    "        \n",
    "        # {0:torch.tensor([0,0,0]) , 1:torch.tensor([0,0,0,0,0])}\n",
    "        for key ,val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "        \n",
    "        # 예측한 bboxes의 confidence socre로 내림차순 정렬\n",
    "        detections.sort(key = lambda x : x[2] , reverse=True)\n",
    "        TP = torch.zeros(len(detections))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Yolov1\n",
    "from dataset import VOCDataset\n",
    "from loss import YoloLoss\n",
    "from utils import intersection_over_union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1edffcdce10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Yolov1\n",
    "from dataset import VOCDataset\n",
    "from utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    intersection_over_union,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "from loss import YoloLoss\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Yolov1\n",
    "from dataset import VOCDataset\n",
    "from utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    intersection_over_union,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "from loss import YoloLoss\n",
    "torch.manual_seed(42)\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "\n",
    "class Compose():\n",
    "    def __init__(self):\n",
    "        self.transforms=transforms.Compose([\n",
    "        transforms.Resize(size=(448,448)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __call__(self , img , bboxes):\n",
    "        img  = self.transforms(img)\n",
    "        return img , bboxes\n",
    "    \n",
    "data_transforms = Compose()\n",
    "\n",
    "def train_fn(train_loader , model , optimizer , loss_fn , device):\n",
    "    mean_loss= []\n",
    "    for batch_idx , (x,y) in tqdm(enumerate(train_loader)):\n",
    "        print(x)\n",
    "        x , y = x.to(device) , y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out , y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss.append(loss.item())\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = Yolov1(split_size = 7 , num_boxes=2 , num_classes=20).to(DEVICE)\n",
    "    loss_fn = YoloLoss()\n",
    "    optimizer = optim.Adam(model.parameters() , lr = LEARNING_RATE , weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "    \n",
    "    train_dataset = VOCDataset(\"./data/train.csv\" , transforms=data_transforms)\n",
    "    test_dataset = VOCDataset(\"./data/test.csv\" , transforms=data_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset , batch_size = BATCH_SIZE , shuffle=True , pin_memory=PIN_MEMORY)\n",
    "    test_loader = DataLoader(test_dataset , batch_size = BATCH_SIZE , shuffle=True , pin_memory=PIN_MEMORY)\n",
    "    \n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4,device=DEVICE\n",
    "        )\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        \n",
    "        train_fn(train_loader, model, optimizer, loss_fn,DEVICE)\n",
    "# if __name__ == \"__main__\":\n",
    "    # main()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Yolov1\n",
    "from dataset import VOCDataset\n",
    "from utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    intersection_over_union,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "from loss import YoloLoss\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "\n",
    "class Compose():\n",
    "    def __init__(self):\n",
    "        self.transforms=transforms.Compose([\n",
    "        transforms.Resize(size=(448,448)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __call__(self , img , bboxes):\n",
    "        img  = self.transforms(img)\n",
    "        return img , bboxes\n",
    "    \n",
    "data_transforms = Compose()\n",
    "train_dataset = VOCDataset(\"./data/train.csv\" , transforms=data_transforms)\n",
    "test_dataset = VOCDataset(\"./data/test.csv\" , transforms=data_transforms)\n",
    "    \n",
    "train_loader = DataLoader(train_dataset , batch_size = BATCH_SIZE , shuffle=True , pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_dataset , batch_size = BATCH_SIZE , shuffle=True , pin_memory=PIN_MEMORY)\n",
    "model = Yolov1(split_size = 7 , num_boxes=2 , num_classes=20).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = next(iter(train_loader))\n",
    "pred = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import intersection_over_union \n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"Calculate loss of yolov1\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "        \n",
    "    def forward(self , predictions , target):\n",
    "        \n",
    "        predictions = predictions.reshape(-1 , self.S , self.S , self.C+self.B*5)\n",
    "      \n",
    "        \n",
    "        iou_b1 = intersection_over_union(predictions[...,21:25], target[...,21:25]) #(N , S ,S , 1)\n",
    "        iou_b2 = intersection_over_union(predictions[...,26:30], target[...,21:25]) #(N , S ,S , 1)\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0) , iou_b2.unsqueeze(0)], dim=0) #(2,N , S ,S , 1)\n",
    "        \n",
    "        \n",
    "        asd , bestbox = torch.max(ious , dim=0)  \n",
    "        exists_box = target[..., 20].unsqueeze(3)\n",
    "        \n",
    "        # box_predictions = exists_box*(   # => (N,S,S,1)\n",
    "            \n",
    "        #     bestbox*predictions[...,26:30] # => (N,S,S,4)\n",
    "           \n",
    "        #     + (1-bestbox)*predictions[...,21:25] # => (N,S,S,4)\n",
    "        # )\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_target = exists_box*target[...,21:25] # => (N,S,S,4)       \n",
    "        \n",
    "        # box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4])+ 1e-6)\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_target[...,2:4] = torch.sqrt(box_target[...,2:4])\n",
    "        \n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions , end_dim=-2) , torch.flatten(box_target , end_dim = -2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #=====================#\n",
    "        #   FOR OBJECT LOSS   #\n",
    "        #=====================#\n",
    "        # confidenc socre\n",
    "        # (N,S,S,1) => (N*S*S,1)\n",
    "        # pred_confidence = exists_box*(bestbox*(predictions[...,25:26]) + (1-bestbox)*(predictions[...,20:21]))\n",
    "        # target_confidence = exists_box*target[...,20:21]\n",
    "        \n",
    "        # object_loss = self.mse(\n",
    "        #     torch.flatten(pred_confidence) , torch.flatten(target_confidence)\n",
    "        # )\n",
    "        \n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "        \n",
    "        #========================#\n",
    "        #   FOR NO OBJECT LOSS   #\n",
    "        #========================#\n",
    "        # (N , S , S , 1) =>(N , S*S*1)\n",
    "        # pred_noobject = (1-exists_box)*(predictions[...,25:26]) + (1-exists_box)*(predictions[...,20:21])\n",
    "        # target_noobject = (1-exists_box)*target[...,20:21] + (1-exists_box)*target[...,20:21] \n",
    "        # no_object_loss = self.mse(\n",
    "        #     torch.flatten(pred_noobject , start_dim=1 ) , torch.flatten(target_noobject , start_dim=1)\n",
    "        # )\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #====================#\n",
    "        #   FOR CLASS LOSS   #\n",
    "        #====================#\n",
    "        \n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box*predictions[...,:20] , end_dim=-2),\n",
    "            torch.flatten(exists_box*target[...,:20] , end_dim=-2)\n",
    "        )\n",
    "        \n",
    "        loss = (\n",
    "            self.lambda_coord*box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_noobj*no_object_loss\n",
    "            + class_loss\n",
    "        )\n",
    "        return loss\n",
    "loss_fn = YoloLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1282.7128, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "loss = loss_fn(pred , label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
